## LLM Judge: Evaluating Model Predictions Against Ground Truth

This project provides a robust framework to evaluate the outputs generated by a fine-tuned Large Language Model (LLM) based on image inputs. By systematically comparing the generated predictions to the actual expected outputs, it enables detailed assessments of the modelâ€™s performance using custom evaluation criteria.